{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv(\"../data/PATIENTS.csv\")\n",
    "\n",
    "#patients.head()\n",
    "#patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(\"../data/disch_notes_small.csv\")\n",
    "\n",
    "#notes.head()\n",
    "#notes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://mimic.mit.edu/docs/iii/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID             0\n",
       "SUBJECT_ID         0\n",
       "GENDER             0\n",
       "DOB                0\n",
       "DOD            30761\n",
       "DOD_HOSP       36546\n",
       "DOD_SSN        33142\n",
       "EXPIRE_FLAG        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.isnull().sum()\n",
    "patients.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(561614, 685200)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notes.TEXT[0]\n",
    "#gensim.utils.simple_preprocess(notes.TEXT[0])\n",
    "discharge_notes = notes.TEXT.apply(gensim.utils.simple_preprocess)\n",
    "discharge_notes\n",
    "# This is defined by the authors, we are recreating the same preprocessing as in the original paper\n",
    "ameModel = gensim.models.Word2Vec(\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    vector_size=100\n",
    ")\n",
    "ameModel.build_vocab(discharge_notes, progress_per=1000)\n",
    "ameModel.epochs\n",
    "ameModel.train(discharge_notes, total_examples=ameModel.corpus_count, epochs=ameModel.epochs)\n",
    "#ameModel.save(\"../data/ameModel.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Analysis and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('infiltrate', 0.9684568643569946),\n",
       " ('gastritis', 0.9672509431838989),\n",
       " ('etiology', 0.9670389294624329),\n",
       " ('part', 0.9662010073661804),\n",
       " ('persistent', 0.964904248714447),\n",
       " ('additionally', 0.961207389831543),\n",
       " ('explanation', 0.9582647085189819),\n",
       " ('multifocal', 0.9579039216041565),\n",
       " ('hemolysis', 0.9575044512748718),\n",
       " ('demand', 0.955880880355835)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ameModel.wv.most_similar(\"ischemia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ameModel.wv.most_similar(\"revascularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pna', 0.9368207454681396),\n",
       " ('hypoxia', 0.9360945224761963),\n",
       " ('complications', 0.9340018033981323),\n",
       " ('ongoing', 0.9335362315177917),\n",
       " ('recovery', 0.9323997497558594),\n",
       " ('complaints', 0.9316456913948059),\n",
       " ('difficulty', 0.9298380613327026),\n",
       " ('exacerbation', 0.9289864301681519),\n",
       " ('secretions', 0.9282615780830383),\n",
       " ('aspiration', 0.928183913230896)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ameModel.wv.most_similar(\"bleeding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ameModel.wv.save(\"../data/discharge_vec.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, dynamic_feature, labels):\n",
    "        self._dynamic_feature = dynamic_feature\n",
    "        self._labels = labels\n",
    "        self._num_examples = 1\n",
    "        self._epoch_completed = 0\n",
    "        self._batch_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if batch_size > self.num_examples or batch_size <= 0:\n",
    "            # raise ValueError('The size of one batch: {} should be less than the total number of '\n",
    "            #                  'data: {}'.format(batcself._labels.shape[0]h_size, self.num_examples))\n",
    "            batch_size = 1\n",
    "        if self._batch_completed == 0:\n",
    "            self._shuffle()\n",
    "        self._batch_completed += 1\n",
    "        start = self._index_in_epoch\n",
    "        if start + batch_size >= self.num_examples:\n",
    "            self._epoch_completed += 1\n",
    "            # TODO 没必要拼接，直接输入剩余部分即可（或者舍弃亦可）\n",
    "            dynamic_rest_part = 0\n",
    "            label_rest_part = 0\n",
    "\n",
    "            self._shuffle()  # 打乱,在一个新的epoch里重新打乱\n",
    "            self._index_in_epoch = 0\n",
    "            return dynamic_rest_part, label_rest_part\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._dynamic_feature[start:end], self._labels[start:end]\n",
    "\n",
    "    def _shuffle(self):  # 打乱\n",
    "        index = np.arange(self._num_examples)\n",
    "        np.random.shuffle(index)\n",
    "        self._dynamic_feature =  0\n",
    "        self._labels = 0\n",
    "\n",
    "    @property\n",
    "    def dynamic_feature(self):\n",
    "        return self._dynamic_feature\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epoch_completed(self):\n",
    "        return self._epoch_completed\n",
    "\n",
    "    @property\n",
    "    def batch_completed(self):\n",
    "        return self._batch_completed\n",
    "\n",
    "    @epoch_completed.setter\n",
    "    def epoch_completed(self, value):\n",
    "        self._epoch_completed = value\n",
    "\n",
    "\n",
    "def read_data(event_type):\n",
    "    dynamic_features = pickle.load(open(\"../data/discharge_vec.pkl\", \"rb\"))\n",
    "    labels = pickle.load(open(\"../data/discharge_vec.pkl\", \"rb\"))\n",
    "    return DataSet(dynamic_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version 2.0.0> change\n",
    "tf.train.AdamOptimizer() => tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pickle import load\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sklearn\n",
    "import xlwt\n",
    "import xlsxwriter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, roc_curve\n",
    "from models import BidirectionalLSTMModel, ContextAttentionRNN, LogisticRegression, CNN, CACNN\n",
    "\n",
    "\n",
    "class ExperimentSetup(object):\n",
    "\n",
    "    kfold = 5\n",
    "    batch_size = 128\n",
    "    hidden_size = 128\n",
    "    epochs = 20\n",
    "    output_n_epochs = 1\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, max_loss=2.0, max_pace=0.01, lasso=0.0, ridge=0.0):\n",
    "        self._learning_rate = learning_rate\n",
    "        self._max_loss = max_loss\n",
    "        self._max_pace = max_pace\n",
    "        self._lasso = lasso\n",
    "        self._ridge = ridge\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @property\n",
    "    def max_loss(self):\n",
    "        return self._max_loss\n",
    "\n",
    "    @property\n",
    "    def max_pace(self):\n",
    "        return self._max_pace\n",
    "\n",
    "    @property\n",
    "    def lasso(self):\n",
    "        return self._lasso\n",
    "\n",
    "    @property\n",
    "    def ridge(self):\n",
    "        return self._ridge\n",
    "\n",
    "    @property\n",
    "    def all(self):\n",
    "        return self._learning_rate, self._max_loss, self._max_pace, self._lasso, self._ridge\n",
    "\n",
    "\n",
    "lr_qx_setup = ExperimentSetup(0.01, 2, 0.0001, 0, 0.001)\n",
    "bi_lstm_qx_setup = ExperimentSetup(0.001, 0.5, 0.01, 0, 0.001)\n",
    "ca_rnn_qx_setup = ExperimentSetup(0.001, 0.08, 0.008, 0, 0.001)\n",
    "\n",
    "lr_xycj_setup = ExperimentSetup(0.01, 2, 0.0001, 0, 0.001)\n",
    "bi_lstm_xycj_setup = ExperimentSetup(0.001, 0.5, 0.001, 0, 0.001)\n",
    "ca_rnn_xycj_setup = ExperimentSetup(0.001, 0.1, 0.0025, 0, 0.001)\n",
    "\n",
    "lr_cx_setup = ExperimentSetup(0.01, 2, 0, 0, 0.001)\n",
    "bi_lstm_cx_setup = ExperimentSetup(0.001, 0.4, 0.04, 0, 0.001)\n",
    "ca_rnn_cx_setup = ExperimentSetup(0.001, 0.1, 0.0025, 0, 0.001)\n",
    "\n",
    "\n",
    "def evaluate(test_index, y_label, y_score, file_name):\n",
    "    \"\"\"\n",
    "    对模型的预测性能进行评估\n",
    "    :param test_index\n",
    "    :param y_label: 测试样本的真实标签 true label of test-set\n",
    "    :param y_score: 测试样本的预测概率 predicted probability of test-set\n",
    "    :param file_name: 输出文件路径    path of output file\n",
    "    \"\"\"\n",
    "    # TODO 全部算完再写入\n",
    "    wb = xlwt.Workbook(file_name + '.xls')\n",
    "    table = wb.add_sheet('Sheet1')\n",
    "    table_title = [\"test_index\", \"label\", \"prob\", \"pre\", \" \", \"fpr\", \"tpr\", \"thresholds\", \" \", \"fp\", \"tp\", \"fn\", \"tn\",\n",
    "                   \"fp_words\", \"fp_freq\", \"tp_words\", \"tp_freq\", \"fn_words\", \"fn_freq\", \"tn_words\", \"tn_freq\", \" \",\n",
    "                   \"acc\", \"auc\", \"recall\", \"precision\", \"f1-score\", \"threshold\"]\n",
    "    for i in range(len(table_title)):\n",
    "        table.write(0, i, table_title[i])\n",
    "\n",
    "    auc = roc_auc_score(y_label, y_score)\n",
    "\n",
    "    threshold = plot_roc(y_label, y_score, table, table_title, file_name)\n",
    "    y_pred_label = (y_score >= threshold) * 1\n",
    "    acc = accuracy_score(y_label, y_pred_label)\n",
    "    recall = recall_score(y_label, y_pred_label)\n",
    "    precision = precision_score(y_label, y_pred_label)\n",
    "    f1 = f1_score(y_label, y_pred_label)\n",
    "\n",
    "    # write metrics\n",
    "    table.write(1, table_title.index(\"auc\"), float(auc))\n",
    "    table.write(1, table_title.index(\"acc\"), float(acc))\n",
    "    table.write(1, table_title.index(\"recall\"), float(recall))\n",
    "    table.write(1, table_title.index(\"precision\"), float(precision))\n",
    "    table.write(1, table_title.index(\"f1-score\"), float(f1))\n",
    "\n",
    "    # collect samples of FP, TP ,FN ,TN and write the result of prediction\n",
    "    fp_sentences = []\n",
    "    fn_sentences = []\n",
    "    tp_sentences = []\n",
    "    tn_sentences = []\n",
    "    fp_count = 1\n",
    "    tp_count = 1\n",
    "    fn_count = 1\n",
    "    tn_count = 1\n",
    "    # sentence_set = load(open(\"resources/all_sentence_progress_note1.pkl\", \"rb\"))\n",
    "    sentence_set = load(open(\"resources/all_sentences_admission_records.pkl\", \"rb\"))\n",
    "    for j in range(len(y_label)):\n",
    "        if y_label[j] == 0 and y_pred_label[j] == 1:  # FP\n",
    "            write_result(j, test_index, y_label, y_score, y_pred_label, table, table_title, sentence_set, fp_sentences,\n",
    "                         \"fp\", fp_count)\n",
    "            fp_count += 1\n",
    "        if y_label[j] == 1 and y_pred_label[j] == 1:  # TP\n",
    "            write_result(j, test_index, y_label, y_score, y_pred_label, table, table_title, sentence_set, tp_sentences,\n",
    "                         \"tp\", tp_count)\n",
    "            tp_count += 1\n",
    "        if y_label[j] == 1 and y_pred_label[j] == 0:  # FN\n",
    "            write_result(j, test_index, y_label, y_score, y_pred_label, table, table_title, sentence_set, fn_sentences,\n",
    "                         \"fn\", fn_count)\n",
    "            fn_count += 1\n",
    "        if y_label[j] == 0 and y_pred_label[j] == 0:  # TN\n",
    "            write_result(j, test_index, y_label, y_score, y_pred_label, table, table_title, sentence_set, tn_sentences,\n",
    "                         \"tn\", tn_count)\n",
    "            tn_count += 1\n",
    "\n",
    "    # word frequency statistic\n",
    "    write_word_frequency(fp_sentences, table, table_title, \"fp\")\n",
    "    write_word_frequency(tp_sentences, table, table_title, \"tp\")\n",
    "    write_word_frequency(fn_sentences, table, table_title, \"fn\")\n",
    "    write_word_frequency(tn_sentences, table, table_title, \"tn\")\n",
    "\n",
    "    wb.save(file_name + \".xls\")\n",
    "\n",
    "\n",
    "def write_result(j, index, y_label, y_score, y_pred_label, table, table_title, sentence_set, samples, group_name,\n",
    "                 count):\n",
    "    \"\"\"\n",
    "    1.write the indexs of test samples and its true label, predicted probabilities and predicted labels\n",
    "    2.collect samples of FP, TP ,FN ,TN\n",
    "    :param j:\n",
    "    :param index:\n",
    "    :param y_label:\n",
    "    :param y_score:\n",
    "    :param y_pred_label:\n",
    "    :param table:\n",
    "    :param table_title:\n",
    "    :param sentence_set:\n",
    "    :param samples:\n",
    "    :param group_name:\n",
    "    :param count:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    table.write(j + 1, table_title.index(\"test_index\"), int(index[j]))\n",
    "    table.write(j + 1, table_title.index(\"label\"), int(y_label[j]))\n",
    "    table.write(j + 1, table_title.index(\"prob\"), float(y_score[j]))\n",
    "    table.write(j + 1, table_title.index(\"pre\"), int(y_pred_label[j]))\n",
    "    samples.extend(sentence_set[index[j]])\n",
    "    table.write(count, table_title.index(group_name), int(index[j]))\n",
    "\n",
    "\n",
    "def write_word_frequency(samples, table, table_title, group_name):\n",
    "    # TODO 命名需改\n",
    "    \"\"\"\n",
    "    词频统计并写入xls文件\n",
    "    :param samples: group of collected samples(samples of FP, TP, FN ,TN)\n",
    "    :param table: the sheet of workbook\n",
    "    :param table_title: the first row of table\n",
    "    :param group_name: name of group\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = Counter(samples).most_common()\n",
    "\n",
    "    for i, word_with_freq in enumerate(words):\n",
    "        word, freq = word_with_freq\n",
    "        table.write(i + 1, table_title.index(group_name + \"_words\"), word)\n",
    "        table.write(i + 1, table_title.index(group_name + \"_freq\"), freq)\n",
    "\n",
    "\n",
    "def plot_roc(test_labels, test_predictions, table, table_title, filename):\n",
    "    \"\"\"\n",
    "    1.plot and save the  ROC curve with AUC value\n",
    "    2.record the FPR, TPR and thresholds\n",
    "    3.choose the threshold with max(TPR-FPR)\n",
    "    :param test_labels: 测试集标签\n",
    "    :param test_predictions: 测试集预测值\n",
    "    :param table: xls文件的sheet\n",
    "    :param table_title: 表头字符串数组\n",
    "    :param filename: 图片文件名\n",
    "    :return: optimal threshold\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, test_predictions, pos_label=1)\n",
    "    threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    for i in range(len(fpr)):\n",
    "        table.write(i + 1, table_title.index(\"fpr\"), fpr[i])\n",
    "        table.write(i + 1, table_title.index(\"tpr\"), tpr[i])\n",
    "        table.write(i + 1, table_title.index(\"thresholds\"), float(thresholds[i]))\n",
    "    table.write(2, table_title.index(\"threshold\"), float(threshold))\n",
    "    auc = \"%.3f\" % sklearn.metrics.auc(fpr, tpr)\n",
    "    title = 'ROC Curve, AUC = ' + str(auc)\n",
    "    with plt.style.context('ggplot'):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(fpr, tpr, \"#000099\", label='ROC curve')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Baseline')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename + '.png', format='png')\n",
    "        plt.close()\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def imbalance_preprocess(train_dynamic, train_y):  # SMOTE过采样\n",
    "    \"\"\"\n",
    "    SMOTE处理不平衡数据集\n",
    "    :param train_dynamic: 训练集输入\n",
    "    :param train_y: 训练集标签\n",
    "    :return: 处理后的训练集输入和标签\n",
    "    \"\"\"\n",
    "    method = SMOTE(kind='regular')\n",
    "    x_res, y_res = method.fit_sample(train_dynamic.reshape([-1, 100 * train_dynamic.shape[1]]), train_y[:, -1])\n",
    "    train_dynamic_res = x_res.reshape([-1, train_dynamic.shape[1], 100])\n",
    "    train_y_res = y_res.reshape([-1, 1])\n",
    "    return train_dynamic_res, train_y_res\n",
    "\n",
    "\n",
    "class LogisticRegressionExperiment(object):\n",
    "    def __init__(self, event_type):\n",
    "        self._event_type = event_type\n",
    "        self._data_set = read_data(event_type)\n",
    "        #print(\"data_set:\", self._data_set.shape)\n",
    "        self._num_features = 1\n",
    "        self._time_steps = 1\n",
    "        self._n_output = 1\n",
    "        print(event_type)\n",
    "        self._model_format()\n",
    "        self._check_path()\n",
    "\n",
    "    def _model_format(self):\n",
    "        if self._event_type == \"qx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = lr_qx_setup.all\n",
    "        elif self._event_type == \"cx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = lr_cx_setup.all\n",
    "        else:\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = lr_xycj_setup.all\n",
    "        self._model = LogisticRegression(num_features=self._num_features,\n",
    "                                         time_steps=self._time_steps,\n",
    "                                         n_output=self._n_output,\n",
    "                                         batch_size=ExperimentSetup.batch_size,\n",
    "                                         epochs=ExperimentSetup.epochs,\n",
    "                                         output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         max_loss=max_loss,\n",
    "                                         max_pace=max_pace,\n",
    "                                         lasso=lasso,\n",
    "                                         ridge=ridge)\n",
    "\n",
    "    def _check_path(self):\n",
    "        if not os.path.exists(\"average_result_cx_TEST\" + self._event_type):\n",
    "            os.makedirs(\"average_result_cx_TEST\" + self._event_type)\n",
    "        self._filename = \"average_result_cx_TEST\" + self._event_type + \"/\" + self._model.name + \" \" + time.strftime(\n",
    "            \"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "    def do_experiments(self):\n",
    "        dynamic_feature = self._data_set.dynamic_feature\n",
    "        labels = self._data_set.labels\n",
    "        kf = sklearn.model_selection.StratifiedKFold(n_splits=ExperimentSetup.kfold, shuffle=False)\n",
    "\n",
    "        n_output =  1 # classes\n",
    "\n",
    "        tol_test_index = np.zeros(shape=0, dtype=np.int32)\n",
    "        tol_pred = np.zeros(shape=(0, n_output))\n",
    "        tol_label = np.zeros(shape=(0, n_output), dtype=np.int32)\n",
    "        i = 1\n",
    "        train_dynamic = dynamic_feature[0]\n",
    "        train_y = labels[0]\n",
    "        #train_dynamic_res, train_y_res = imbalance_preprocess(train_dynamic, train_y)  # SMOTE过采样方法处理不平衡数据集\n",
    "\n",
    "        test_dynamic = dynamic_feature[0]\n",
    "        test_y = labels[0]\n",
    "\n",
    "        train_set = DataSet(0, 0)\n",
    "        test_set = DataSet(test_dynamic, test_y)\n",
    "\n",
    "        self._model.fit(train_set, test_set, self._event_type)\n",
    "\n",
    "        y_score = self._model.predict(test_set)\n",
    "\n",
    "        tol_test_index = np.concatenate((tol_test_index, 0))\n",
    "        tol_pred = np.vstack((tol_pred, y_score))\n",
    "        tol_label = np.vstack((tol_label, test_y))\n",
    "        print(\"Cross validation: {} of {}\".format(i, ExperimentSetup.kfold),\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "        i += 1\n",
    "        evaluate(tol_test_index, tol_label, tol_pred, self._filename)\n",
    "        self._model.close()\n",
    "\n",
    "\n",
    "class BidirectionalLSTMExperiments(LogisticRegressionExperiment):\n",
    "    def __init__(self, event_type):\n",
    "        super().__init__(event_type)\n",
    "\n",
    "    def _model_format(self):\n",
    "        if self._event_type == \"qx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = bi_lstm_qx_setup.all\n",
    "        elif self._event_type == \"cx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = bi_lstm_cx_setup.all\n",
    "        else:\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = bi_lstm_xycj_setup.all\n",
    "        self._model = BidirectionalLSTMModel(num_features=self._num_features,\n",
    "                                             time_steps=self._time_steps,\n",
    "                                             lstm_size=ExperimentSetup.hidden_size,\n",
    "                                             n_output=self._n_output,\n",
    "                                             batch_size=ExperimentSetup.batch_size,\n",
    "                                             epochs=ExperimentSetup.epochs,\n",
    "                                             output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             max_loss=max_loss,\n",
    "                                             max_pace=max_pace,\n",
    "                                             lasso=lasso,\n",
    "                                             ridge=ridge)\n",
    "\n",
    "\n",
    "class ContextAttentionRNNExperiments(LogisticRegressionExperiment):\n",
    "    def __init__(self, event_type):\n",
    "        super().__init__(event_type)\n",
    "\n",
    "    def _model_format(self):\n",
    "        if self._event_type == \"qx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_qx_setup.all\n",
    "        elif self._event_type == \"cx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_cx_setup.all\n",
    "        else:\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_xycj_setup.all\n",
    "        self._model = ContextAttentionRNN(num_features=self._num_features,\n",
    "                                          time_steps=self._time_steps,\n",
    "                                          lstm_size=ExperimentSetup.hidden_size,\n",
    "                                          n_output=self._n_output,\n",
    "                                          batch_size=ExperimentSetup.batch_size,\n",
    "                                          epochs=ExperimentSetup.epochs,\n",
    "                                          output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                                          learning_rate=learning_rate,\n",
    "                                          max_loss=max_loss,\n",
    "                                          max_pace=max_pace,\n",
    "                                          lasso=lasso,\n",
    "                                          ridge=ridge)\n",
    "\n",
    "    def attention_analysis(self):\n",
    "        dynamic_feature = self._data_set.dynamic_feature\n",
    "        labels = self._data_set.labels\n",
    "        kf = sklearn.model_selection.StratifiedKFold(n_splits=ExperimentSetup.kfold, shuffle=True)\n",
    "        # models = [\"qx_case1_save_net06-20-10-54.ckpt\", \"qx_case1_save_net06-20-10-58.ckpt\",\n",
    "        #           \"qx_case1_save_net06-20-11-03.ckpt\", \"qx_case1_save_net06-20-11-08.ckpt\",\n",
    "        #           \"qx_case1_save_net06-20-11-13.ckpt\"]\n",
    "        # models = [\"xycj_case1_save_net06-20-11-45.ckpt\", \"xycj_case1_save_net06-20-11-51.ckpt\",\n",
    "        #           \"xycj_case1_save_net06-20-11-56.ckpt\", \"xycj_case1_save_net06-20-12-01.ckpt\",\n",
    "        #           \"xycj_case1_save_net06-20-12-06.ckpt\"]\n",
    "        models = [\"cx_case1_save_net06-20-11-19.ckpt\", \"cx_case1_save_net06-20-11-24.ckpt\",\n",
    "                  \"cx_case1_save_net06-20-11-29.ckpt\", \"cx_case1_save_net06-20-11-35.ckpt\",\n",
    "                  \"cx_case1_save_net06-20-11-40.ckpt\"]\n",
    "        i_fold = 1\n",
    "        test_idx_tol = np.zeros(0, dtype=np.int32)\n",
    "        attention_signals_tol = np.zeros(shape=(0, 80, 10))\n",
    "        test_labels_tol = np.zeros([0, 1])\n",
    "        pre_tol = np.zeros([0, 1])\n",
    "        for train_idx, test_idx in kf.split(X=dynamic_feature, y=labels.reshape(-1)):  # 五折交叉\n",
    "            test_dynamic = dynamic_feature[test_idx]\n",
    "            test_labels = labels[test_idx]\n",
    "            prob, attention_signals = self._model.attention_analysis(test_dynamic, models[i_fold - 1])\n",
    "            fpr, tpr, thresholds = roc_curve(test_labels, prob, pos_label=1)\n",
    "            threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "            pre = (prob >= threshold) * 1\n",
    "            attention_signals_tol = np.concatenate((attention_signals_tol, attention_signals))\n",
    "            test_idx_tol = np.concatenate((test_idx_tol, test_idx))\n",
    "            test_labels_tol = np.concatenate((test_labels_tol, test_labels))\n",
    "            pre_tol = np.concatenate((pre_tol, pre))\n",
    "            i_fold += 1\n",
    "        # break\n",
    "        attention_write(attention_signals_tol, test_idx_tol, dynamic_feature.shape[1], test_labels_tol, pre_tol)\n",
    "        self._model.close()\n",
    "\n",
    "\n",
    "def dict_form(sentence_set, test_index, sample, attention_signals_avg, dict):\n",
    "    # dict_temp = {}\n",
    "    # for step, word in enumerate(sentence_set[test_index[sample]]):\n",
    "    #     if word in dict_temp:\n",
    "    #         dict_temp[word].append(attention_signals_avg[sample, step])\n",
    "    #     else:\n",
    "    #         dict_temp[word] = [attention_signals_avg[sample, step]]\n",
    "    # for (d, v) in dict_temp.items():\n",
    "    #     if d in dict:\n",
    "    #         dict[d].append(np.sum(dict_temp[d]))\n",
    "    #     else:\n",
    "    #         dict[d] = [np.sum(dict_temp[d])]\n",
    "    for step, word in enumerate(sentence_set[test_index[sample]]):\n",
    "        if word in dict:\n",
    "            dict[word].append(attention_signals_avg[sample, step])\n",
    "        else:\n",
    "            dict[word] = [attention_signals_avg[sample, step]]\n",
    "\n",
    "\n",
    "def attention_write(attention_signals, test_index, n_steps, labels, pre):\n",
    "    attention_signals_shift = np.zeros([len(test_index), n_steps, n_steps + 10])\n",
    "    for i_sample in range(len(test_index)):\n",
    "        for j in range(n_steps):\n",
    "            attention_signals_shift[i_sample, j, j:j + 5] = attention_signals[i_sample, j, 0:5]\n",
    "            attention_signals_shift[i_sample, j, j + 6:j + 11] = attention_signals[i_sample, j, 5:10]\n",
    "    attention_signals_avg = np.sum(attention_signals_shift, 1)[:, 5:n_steps + 5] / 10\n",
    "    # sentence_set = load(open(\"resources/all_sentences_progress_notes.pkl\", \"rb\"))\n",
    "    sentence_set = load(open(\"resources/all_sentences_admission_records.pkl\", \"rb\"))\n",
    "    workbook = xlsxwriter.Workbook(\"cx_case1_attention_all_average.xlsx\")\n",
    "    table = workbook.add_worksheet()\n",
    "    dict_fp = {}\n",
    "    dict_tp = {}\n",
    "    dict_fn = {}\n",
    "    dict_tn = {}\n",
    "    for sample in range(len(test_index)):\n",
    "        if labels[sample] == 0 and pre[sample] == 1:\n",
    "            dict_form(sentence_set, test_index, sample, attention_signals_avg, dict_fp)\n",
    "        if labels[sample] == 1 and pre[sample] == 1:\n",
    "            dict_form(sentence_set, test_index, sample, attention_signals_avg, dict_tp)\n",
    "        if labels[sample] == 1 and pre[sample] == 0:\n",
    "            dict_form(sentence_set, test_index, sample, attention_signals_avg, dict_fn)\n",
    "        if labels[sample] == 0 and pre[sample] == 0:\n",
    "            dict_form(sentence_set, test_index, sample, attention_signals_avg, dict_tn)\n",
    "\n",
    "    for (d, v) in dict_fp.items():\n",
    "        dict_fp[d] = np.average(dict_fp[d])\n",
    "    dict_fp = sorted(dict_fp.items(), key=lambda d: d[1], reverse=True)\n",
    "    for (d, v) in dict_tp.items():\n",
    "        dict_tp[d] = np.average(dict_tp[d])\n",
    "    dict_tp = sorted(dict_tp.items(), key=lambda d: d[1], reverse=True)\n",
    "    for (d, v) in dict_fn.items():\n",
    "        dict_fn[d] = np.average(dict_fn[d])\n",
    "    dict_fn = sorted(dict_fn.items(), key=lambda d: d[1], reverse=True)\n",
    "    for (d, v) in dict_tn.items():\n",
    "        dict_tn[d] = np.average(dict_tn[d])\n",
    "    dict_tn = sorted(dict_tn.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    for i, d in enumerate(dict_fp):\n",
    "        table.write(i + 1, 1, d[0])\n",
    "        table.write(i + 1, 2, d[1])\n",
    "    for i, d in enumerate(dict_tp):\n",
    "        table.write(i + 1, 4, d[0])\n",
    "        table.write(i + 1, 5, d[1])\n",
    "    for i, d in enumerate(dict_fn):\n",
    "        table.write(i + 1, 7, d[0])\n",
    "        table.write(i + 1, 8, d[1])\n",
    "    for i, d in enumerate(dict_tn):\n",
    "        table.write(i + 1, 10, d[0])\n",
    "        table.write(i + 1, 11, d[1])\n",
    "    workbook.close()\n",
    "\n",
    "\n",
    "class CNNExperiments(LogisticRegressionExperiment):\n",
    "    def __init__(self, event_type):\n",
    "        super().__init__(event_type)\n",
    "\n",
    "    def _model_format(self):\n",
    "        if self._event_type == \"qx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_qx_setup.all\n",
    "        elif self._event_type == \"cx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_cx_setup.all\n",
    "        else:\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_xycj_setup.all\n",
    "        self._model = CNN(num_features=self._num_features,\n",
    "                          time_steps=self._time_steps,\n",
    "                          lstm_size=ExperimentSetup.hidden_size,\n",
    "                          n_output=self._n_output,\n",
    "                          batch_size=ExperimentSetup.batch_size,\n",
    "                          epochs=ExperimentSetup.epochs,\n",
    "                          output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                          learning_rate=learning_rate,\n",
    "                          max_loss=max_loss,\n",
    "                          max_pace=max_pace,\n",
    "                          lasso=lasso,\n",
    "                          ridge=ridge)\n",
    "\n",
    "\n",
    "class CACNNExperiments(LogisticRegressionExperiment):\n",
    "    def __init__(self, event_type):\n",
    "        super().__init__(event_type)\n",
    "\n",
    "    def _model_format(self):\n",
    "        if self._event_type == \"qx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_qx_setup.all\n",
    "        elif self._event_type == \"cx\":\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_cx_setup.all\n",
    "        else:\n",
    "            learning_rate, max_loss, max_pace, lasso, ridge = ca_rnn_xycj_setup.all\n",
    "        self._model = CACNN(num_features=self._num_features,\n",
    "                            time_steps=self._time_steps,\n",
    "                            lstm_size=ExperimentSetup.hidden_size,\n",
    "                            n_output=self._n_output,\n",
    "                            batch_size=ExperimentSetup.batch_size,\n",
    "                            epochs=ExperimentSetup.epochs,\n",
    "                            output_n_epoch=ExperimentSetup.output_n_epochs,\n",
    "                            learning_rate=learning_rate,\n",
    "                            max_loss=max_loss,\n",
    "                            max_pace=max_pace,\n",
    "                            lasso=lasso,\n",
    "                            ridge=ridge)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qx\n",
      "learning_rate= 0.001 max_loss= 0.08 max_pace= 0.008 lasso= 0 ridge= 0.001\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB6DF01C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB6DF01C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB6DF01C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB6DF01C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB6DF4988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB6DF4988>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB6DF4988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB6DF4988>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB74A4E88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB74A4E88>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB74A4E88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020CB74A4E88>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB47F5FC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB47F5FC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB47F5FC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020CB47F5FC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "CA-RNN/Variable:0\n",
      "CA-RNN/fully_connected/weights:0\n",
      "CA-RNN/fully_connected/biases:0\n",
      "CA-RNN/bidirectional_rnn/fw/basic_lstm_cell/kernel:0\n",
      "CA-RNN/bidirectional_rnn/fw/basic_lstm_cell/bias:0\n",
      "CA-RNN/bidirectional_rnn/bw/basic_lstm_cell/kernel:0\n",
      "CA-RNN/bidirectional_rnn/bw/basic_lstm_cell/bias:0\n",
      "CA-RNN/fully_connected_1/weights:0\n",
      "CA-RNN/fully_connected_1/biases:0\n",
      "auc\tepoch\tloss\tloss_diff\tcount\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'CA-RNN/input:0', which has shape '(?, 1, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_63480\\2090471016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# MultiLayerPercptronExperimrnt(revascularization).do_experiments()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mContextAttentionRNNExperiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mischemia\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# BidirectionalLSTMExperiments(ischemia).do_experiments()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_63480\\647048965.py\u001b[0m in \u001b[0;36mdo_experiments\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dynamic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saara\\Documents\\MLProject\\CNN\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data_set, test_set, event_type)\u001b[0m\n\u001b[0;32m    274\u001b[0m             self._sess.run(self._train_op, feed_dict={self._x: dynamic_feature,\n\u001b[0;32m    275\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                                                       self._v: self._template})\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_completed\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_n_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_completed\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogged\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlh-ame-detection\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlh-ame-detection\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1149\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1150\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'CA-RNN/input:0', which has shape '(?, 1, 1)'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # TODO此处字符串改为constant\n",
    "    ischemia = \"qx\"\n",
    "    bleeding = \"cx\"\n",
    "    revascularization = \"xycj\"\n",
    "\n",
    "    for i in range(5):\n",
    "        # LogisticRegressionExperiment(ischemia).do_experiments()\n",
    "        # MultiLayerPercptronExperimrnt(ischemia).do_experiments()\n",
    "        #\n",
    "        # LogisticRegressionExperiment(bleeding).do_experiments()\n",
    "        # MultiLayerPercptronExperimrnt(bleeding).do_experiments()\n",
    "        #\n",
    "        # LogisticRegressionExperiment(revascularization).do_experiments()\n",
    "        # MultiLayerPercptronExperimrnt(revascularization).do_experiments()\n",
    "        tf.reset_default_graph()\n",
    "        ContextAttentionRNNExperiments(ischemia).do_experiments()\n",
    "        # BidirectionalLSTMExperiments(ischemia).do_experiments()\n",
    "\n",
    "        ContextAttentionRNNExperiments(bleeding).do_experiments()\n",
    "        # BidirectionalLSTMExperiments(bleeding).do_experiments()\n",
    "\n",
    "        ContextAttentionRNNExperiments(revascularization).do_experiments()\n",
    "        # BidirectionalLSTMExperiments(revascularization).do_experiments()\n",
    "\n",
    "        # ContextAttentionRNNWithOriginExperiments(revascularization).do_experiments()\n",
    "        # ContextAttentionRNNWithOriginExperiments(bleeding).do_experiments()\n",
    "\n",
    "        # CNNExperiments(ischemia).do_experiments()\n",
    "        # CNNExperiments(revascularization).do_experiments()\n",
    "        # CNNExperiments(bleeding).do_experiments()\n",
    "        #\n",
    "        # CACNNExperiments(ischemia).do_experiments()\n",
    "        # CACNNExperiments(revascularization).do_experiments()\n",
    "        # CACNNExperiments(bleeding).do_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "All credit goes to the original authors and the code repository which can be found below\n",
    "\n",
    "Repository: \"Using neural attention networks to detect adverse medical events from\n",
    "electronic health records\"\n",
    "Code: https://github.com/ZJU-BMI/AME-detection"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c83f15b666c35b3184d6bfc342fb94f9175434fb53d4a80ba73024ed92ffdc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
